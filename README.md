## OpenDatacube implementation

### Defaults

* sentinel1_metadata.yaml : sentinel1 metadata.

* Sentinel1_product_definition.yaml : sentinel1 product definition.

### Utility scripts

* dhusget   : script to download whole datasets from scihub.copernicus.eu.

* s1prepare : script to prepare preproccessed images to be indexable through the datacube api

### Setup - Implementation so far

1. First configure the datacube [Datacube installation](https://github.com/ceos-seo/data_cube_ui/blob/master/docs/open_data_cube_install.md)
2. Define the product and the metadata through the use of the datacube api
3. Test the database with a simple jupyter notebook.

### Activation

* Run the script under /PATH_OF_YOUR_DATACUBE_INSTALLATION/datacube_env/bin/activate to get access to the virtualenv

### Problems - Observations

* The path of the images-bands is incorrectly annotated if the rasters are in a subfolder.

* Final yaml is saved as *agdc-metadata.yaml* on all inputs

* The preparation script requires rasters to be on a folder called *SENTINEL_1A* (configurable through the script).

### TODO

* Implement recursive folder search to correct annotate the rasters' path.

* Provide example queries for simple operations.

### Usage - Scenario

#### Adding product definitions
```bash
datacube product add <path-to-product-definition.yaml>
```
> Examples of product definition can be found [here](https://datacube-core.readthedocs.io/en/latest/ops/indexing.html#dataset-documents)

#### Adding metadata for the desired product
```bash
datacube metadata add <path-to-metadata-definition.yaml>
```

#### Prepare the preproccessed .tif files
```bash
python <path-to-prepare-script.py> <path-to-folder-containing-tif-files>
```

#### Add the dataset generated by the prepare script
```bash
datacube dataset add <path-to-yaml-generated-from-script>
```



